# 我的分數比你的分數高嗎？相依樣本與獨立樣本 {#two-samples}

經過第\@ref(one-sample)與第\@ref(ci)單元，我們已經建立一個心理科學的統計實務印象：我們能相信某個研究結果，是建立在多次隨機程序的調查與實驗。傳統統計書很少說明這點，因為介紹假設檢定的同時通常會一併介紹**型一錯誤(Type 1 error)**與**型二錯誤(Type 2 error)**。由於許多時候預估的型一錯誤等於預先宣告的顯著水準，許多初學者會以為*p*值就是這項研究的型一錯誤，甚至許多資深學者，包括可能是曾經教過你統計的老師也誤以為如此[@GreenlandStatisticaltestsvalues2016; @LakensUnderstandingcommonmisconceptions2017; @NelsonInterpretationSignificanceLevels1986]。

心理科學最常處理的統計實務是兩個平均數的比較，所以我們在這個單元裡好好討論什麼是型一錯誤與型二錯誤，還有該如何正確地分析及推論。

## 型一錯誤與型二錯誤

不管你對隨機樣本的認識程度是好是壞，看到圖\@ref(fig:type-1-type-2)就能大致理解什麼是型一錯誤與型二錯誤[@EllisEssentialGuideEffect2010]。用圖中的醫師誤診進一步說明：型一錯誤是病人沒病，但是醫師診斷為有病，又稱為偽陽性結論(False Positive)；型二錯誤是病人有病，但是醫師診斷為沒有病，又稱為偽陰性結論(False Negative)。我們把醫師換成研究者，病人換成資料，就可以描述心理科學研究結論出錯的狀況。

```{r type-1-type-2, out.width='80%', fig.cap="型一錯誤(偽陽性結論，false positive)與型二錯誤(偽陰性結論，false negative)。", fig.align='center', echo=FALSE}
knitr::include_graphics("images/type-i-and-type-ii-errors.jpg")
```

**次數主義統計學(Frequentism Statistics)**運用第\@ref(randomization)單元示範的重覆抽樣概念，看待型一錯誤與型二錯誤。一項研究結論有顯著結果，或者說陽性結果，經過數次再現，會累積數次顯著與不顯著的結果，不顯著結果的比例是**型一錯誤率**。相對地，另一項沒有顯著結果的研究結論，或者說陰性結果，經過數次再現，也會累積數次顯著與不顯著的結果，顯著結果的比例是**型二錯誤率**。

因為隨機變數的數學原理加上資料模擬方法，多數統計實務只要根據一次研究結果，就可以估計這一項研究的型一錯誤率與型二錯誤率。我們用第\@ref(one-sample)單元的示範資料，配合圖\@ref(fig:type-1-type-2-app)的互動說明，可以幫助你理解型一錯誤率與型二錯誤率，和樣本數目、事先設定的顯著水準，以及考驗力的關係。

根據第\@ref(one-sample)單元的資訊，我們知道那12位大學生的壓力感受分數，以.05的顯著水準評估，單尾與雙尾檢定都顯示與原始報告有顯著差異，估計效果量是-0.696。現在請你把圖\@ref(randomization)裡的`Power`勾起來，接著把`Significance level`、`Sample Size`、`Effect Size`調整為0.05、12、0.7。這些設定比照示範案例的分析結果，請輪流點擊`one-tailed`與`two-tailed`，看看圖下面呈現的`Power`與第\@ref(one-sample)單元預估的差異。

```{r type-1-type-2-app, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE, fig.cap="型一與型二錯誤率互動解析。如果無法顯示，請關閉瀏覽器封鎖不安全網頁功能。"}
knitr::include_url("http://rpsychologist.com/d3/NHST/", 
  height = "1500px")
```

因為圖\@ref(fig:type-1-type-2-app)的`H_0_`與`H_a_`是常態分佈，所以得到的考驗力估計值與我們用[WebPower](https://webpower.psychstat.org/qanda/)的估計有些差異，不過依然是單尾檢定的考驗力大於雙尾檢定的考驗力。請你仔細看圖下方的**型一錯誤率**數值，是不是與顯著水準一致？**型二錯誤率**與考驗力加起來是不是100%？

現在你可以知道**型二錯誤率**與考驗力來源相同，都是對立假設抽樣分配的累積機率函數輸出結果。型二錯誤率又稱為偽陰率，而考驗力又稱為正陽率。因此一項理想的研究不只有顯著結果，考驗力也要越高越好。

那麼**型一錯誤率**等於顯著水準該怎麼解釋？現在我們設定之後的再現研究維持同樣的樣本數與考驗力，只是每次得到的效果量都不同，實際的顯著水準會不會改變？請你改勾選圖\@ref(fig:type-1-type-2-app)的`alpha`，調整`effecti size`，觀察型一錯誤率如何變化？

你會發現效果量越小，型一錯誤率越高。反之，效果量越大，型一錯誤率越低。還記得第\@ref(ci)單元曾經介紹的**捕獲百分比**是個浮動的數值，型一錯誤率、乃至型二錯誤率與考驗力也是如此。因為就算真的存在母群平均值，抽樣分配會因為樣本數導致變異趨勢的變化，進而改變所有累積機率函數的輸出值。面對這些事實，我們就能理解心理科學研究要開放資料提供眾人核實的重要性。

## 示範資料說明

在此使用兩項研究的公開資料，學習相依樣本與獨立樣本t檢定分析的程序，這兩筆資料都是預先註冊實驗的成果[@WagenmakersBayesianinferencepsychology2017; @ZwaanParticipantNonnaivetereproducibility2017]。

Zwaan等人[-@ZwaanParticipantNonnaivetereproducibility2017]挑選九個知名的認知心理學實驗，透過網路邀請參與者相隔幾天，進行同一項實驗兩次。所有實驗都是**參與者內設計**，是實驗操作的兩種情況，每位參與者在實驗過程中都會經歷。稍後要示範分析的**語義促發效應**，是一種詞彙判斷作業的反應時間測量結果。詞彙判斷作業是請參與者判斷現在呈現的目標詞是真詞還是假詞，例如hand是真詞，hamq是假詞。每個目標詞之前會隨機呈現一個與目標詞有關聯的詞或一個無關聯的詞，例如與hand有關聯的foot。過去許多研究發現先呈現有關聯的詞，參與者判斷真詞的反應會比先呈現無關聯的詞快，有無關聯之間的反應時間差異就是**語義促發效應**。我們將使用[公開資料](https://osf.io/atfcj/)示範相依樣本t檢定分析程序。

JASP開發團隊執行長Wagenmakers與同事們[-@WagenmakersBayesianinferencepsychology2017]嘗試再現德國社會心理學者Topolinski與Sparenberg[-@TopolinskiTurningHandsTime2012]的體現認知實驗，公開資料做為獨立樣本t檢定的示範。Topolinski與Sparenberg的實驗是請參與者來實驗室進行12題開放態度量表，題目內容像是「我經常嘗試新奇的外國食物」，以五點量表表達自己對題目內容的開放程度（-2:非常不同意；2:非常同意）。實驗室環境安排如圖\@ref(fig:roll-setting)，102位參與者答題時要依指示捲動面前的衛生紙捲，其中48位以順時針方向轉動(clockwise)，另外54位以逆時針方向轉動(counter-clockwise)。

```{r roll-setting, out.width='60%', fig.cap="Wagenmakers等人(2017)再現Topolinski與Sparenberg(2012)體現認知實驗的現場設置。", fig.align='center', echo=FALSE}
knitr::include_graphics("images/Wagenmakers-exp-setting.jpeg")
```

Topolinski與Sparenberg的假設是大多數可轉動的工具開啟方向都是順時針，順時針捲動衛生紙卷的參與者，回答開放態度量表的問題，給的評分應該比逆時針捲動的參與者高。原始研究確實發現顯著較高分的結果，再現研究結果則存放於JASP的示範資料集`Kitchen Rolls`。在獨立樣本t檢定的示範分析，我們將知道原始研究結果有沒有成功再現。

## 相依樣本與重覆量數

我們以jamovi示範相依樣本t檢定，示範檔案[由此下載](https://osf.io/j5xpa/)。其中會用於分析的資料欄位是`session1_related`，`session1_unrelated`，`session2_related`，`session2_unrelated`，都是同一位參與者在每個實驗情況的平均反應時間。

心理學與許多行為科學有兩種研究設計會用相依樣本t檢定分析：一種是前後測量，常見於教育與醫學研究，這種設計讓參與者在研究者想測試的教學方法或醫療處置前後，測量預期會有改變的指標。另一種是**重覆量數**，如同即將示範的語義促發效應，有兩種不同條件的測量狀況，在一系列題目之間隨機呈現。雖然分析方法沒有太大差異，設計背後想探討的問題本質並不一樣。一個關鍵差異是前後測量的問題類以單樣本平均數，會有一個非零的數值(前測結果）做為虛無假設抽樣分配的平均值。另一方面，使用重覆量數設計的問題，都是非有即無的預測，所以通常設定平均數為零的虛無假設分佈。

### 分析前的宣告

因為語義促發效果已經累積許多研究結果，我們可以預測無關聯性的反應時間顯著慢於有關聯性的反應時間。Zwaan等人不同於前人之處是讓參與者做了兩次程序一樣的實驗，所以我們要分別分析第一次與第二次的資料，宣告假設檢定與信賴區間的可能結果：

> 如果第一次與第二次實驗確實顯示語義促發效應，反應時間差異的單尾t檢定應該出現小於.01的*p*值，而且99%信賴區間不會包括0。

此外，重覆量數的假設檢定是以平均數為0的t分佈做為抽樣分配，我們在第\@ref(one-sample)認識t分佈時，已經透過互動操作知道樣本數目超過30，t分佈整體接近常態分佈。所以這筆超過一百位參與者的資料，可以假設接近常態分佈，這讓我們可以使用已經內建於統計軟體的檢驗功能，了解這筆資料符合常態分佈的程度。如果有常態分佈的檢驗資訊，根據主要宣言做出的結論會更有說服力。**預先註冊**也鼓勵研究者事先說明想探索與主要目的無直接相關的問題，所以我們可以再加一則宣告，做為檢驗資料是否符合**常態分佈假設**的但書：

> 如果第一次與第二次實驗資料都符合**常態分佈假設**，Shapiro-Wilk檢驗的*p*值不會小於.05。如果出現小於.05的*p*值，必須在報告中討論。

### 示範檔案操作說明

開啟[jamovi示範檔](https://osf.io/j5xpa/)，報表標題指示這是以`T-Tests`模組的`Paired Samples T-Test`製作。圖\@ref(fig:jamovi-paired-t-operation)是根據分析前宣告，勾選與調整的信賴水準。

```{r jamovi-paired-t-operation, out.width='80%', fig.cap="jamovi相依樣本單尾t檢定分析操作設定示範。", fig.align='center', echo=FALSE}
knitr::include_graphics("images/jamovi-paired-t-operation.jpg")
```

### 示範檔案報表解讀

```{r jamovi-paired-t-result, out.width='80%', fig.cap="jamovi相依樣本單尾t檢定報表示範。", fig.align='center', echo=FALSE}
knitr::include_graphics("images/jamovi-paired-t-result.jpg")
```

我們從圖\@ref(fig:jamovi-paired-t-result)來看如何從報表擷取要報告的資訊。最上面的表格有兩次結果的t檢定與信賴區間資料，可比照單一樣本平均數的報表擷取符合報告格式的資訊，以分析前宣告的標準，這項研究兩次實驗都有發現顯著的語義促發效果。中間是**常態分佈假設**的檢定結果，根據分析前宣告的標準，第一次實驗資料並不符合常態分佈，我們可以在報告統計資訊之後，討論造成兩次實驗差異原因，以及對實驗結果的影響。最後的描述統計表格，都是在報告中與t檢定統計資訊放置於文字呈現。

### 分析後報告

因為有兩次實驗，我建議分別報告讓人更容易閱讀：

第一次實驗的語義促發效果有26.62毫秒(有關聯：M = 528.36, SD = 69.97；無關聯：M = 555.98, SD = 72.17， 99% CI [19.833 30.414])，達到事先宣告的統計顯著水準，*t*(159) = 10.22, *p* < .001, *d* = 0.81。

第二次實驗的語義促發效果有22.85毫秒(有關聯：M = 520.60, SD = 65.83；無關聯：M = 543.45, SD = 66.33， 99% CI [17.854 27.837])，達到事先宣告的統計顯著水準，*t*(159) = 11.93, *p* < .001, *d* = 0.94。

由次分析前宣告已聲明如果有違反常態分佈假設的結果，就要在報告統計資訊後討論，這裡建議兩個方向，一個是從語義促發效果的文獻資訊來談，但是這個方向已經超出基礎統計的範圍。另一個方向是比對第一次實驗與第二次實驗的估計標準誤(SE)，你也許有注意到第二次的估計標準誤比第一次的小，或許讓參與者再做一次，反應時間的變異會變小，讓資料分配接近常態分佈。至次為何會變小的原因，就看你認為值不值得當成一個新的研究主題。

## 獨立樣本與組間比較

我們以JASP示範獨立樣本t檢定，示範檔案[由此下載](https://osf.io/kw6vy/)。其中會用於分析的資料欄位是`mean_NEO`與`Rotation`。`mean_NEO`是每位參與者的所有答題平均評分，`Rotation`是依捲動衛生紙捲方向的分組標記。

獨立組比較研究通常希望收集的資料各組樣本數目相同，但是通常不能盡如人意。就像這則示範案例，Wagenmakers計畫收集兩組各50人，最後因為一些無法避免的意外與必須遵守事先約定的隨機原則，最後可分析的順時針與逆時針的參與者人數各為48與54。幸好獨立組t檢定有樣本數不相等的版本，而且現在的統計軟體都能自動判斷該用那種版本。因為樣本數不相等的t檢定自由度計算公式複雜，使用軟體分析資料勢在必行。而研究者的責任是確保收集到的資料能歸納出有效的結論，了解研究設計與分析程序的侷限，像是兩組資料變異是否相等(equalityo of variance)，在各組樣本數目不同的狀況是值得注意的細節。

### 分析前的宣告

Wagenmakers決定執行研究之前，只有Topolinski與Sparenberg發表的結果，無法確定這次結果會不會一樣，因此適合以母群平均數尚待確認的立場，設定宣告條件。兩組平均數經過假設檢定與信賴區間的分析，可能結果會是：

> 如果捲動衛生紙捲的方向會影響參與者的開放態度，兩組平均數的差異雙尾t檢定應該出現小於.05的*p*值，而且95%信賴區間不會包括0。

至於兩組變異數，我們當然期望是一致的。獨立組設計也預先假設兩組資料彼此獨立，抽樣分配都符合常態分佈。所以我們可以另外宣告**變異等量假設**與**常態分佈假設**的分析標準：

> (1)如果兩組資料符合**變異等量假設**，Levene檢驗的*p*值應該大於.05。(2)如果兩組資料符合**常態分佈假設**，Shapiro-Wilk檢驗的*p*值應該大於.05。如果任何一項出現小於.05的*p*值，必須在報告中討論。

### 示範檔案操作說明

開啟[JASP示範檔案](https://osf.io/kw6vy/)，點擊報表你會看到`T-Tests`的`Independent Samples T-Test`功能選單。圖\@ref(fig:JASP-independent-t-operation)是根據分析前宣告，勾選與調整的信賴水準。

```{r JASP-independent-t-operation, out.width='80%', fig.cap="JASP獨立樣本雙尾t檢定分析操作設定示範。", fig.align='center', echo=FALSE}
knitr::include_graphics("images/JASP-independent-t-operation.jpg")
```

### 示範檔案報表解讀

```{r JASP-independent-t-result, out.width='80%', fig.cap="JASP獨立樣本雙尾t檢定報表示範。", fig.align='center', echo=FALSE}
knitr::include_graphics("images/JASP-independent-t-result.jpg")
```

我們從圖\@ref(fig:JASP-independent-t-result)來看如何從報表擷取要報告的資訊。最上面表格裡的t檢定與信賴區間資料，可比照前面的案例擷取符合報告格式的資訊，以分析前宣告的標準，這項研究並沒有發現顯著差異。中間是**變異等量假設**與**常態分佈假設**的檢定結果，根據分析前宣告的標準，兩者都符合假設。最後的描述統計表格，都是在報告中與t檢定統計資訊放置於文字呈現。

### 分析後報告

這次實驗沒有發現顯著差異，如果是傳統的教學或研究習慣，會簡略報告。但是因為有預先宣告，報告約定要呈現的資訊，才是負責的研究態度。報告寫作範例如下：

順時針捲動的平均評分比逆時針捲動的平均評分高0.072分(順時針：M = 0.641, SD = 0.496；無關聯：M = 0.713, SD = 0.473， 95% CI [-0.118 0.263])，並未達到事先宣告的統計顯著水準，*t*(100) = 0.754, *p* = .453, *d* = 0.149。

因為事前宣告已言明如果通過檢驗標準，就沒有違反**變異等量假設**與**常態分佈假設**的問題，在報告中就沒有討論的必要。

## 效果量、p值與考驗力

Wagenmakers等人的再現研究雖然沒有發現顯著結果，但是給我們一個深入探討效果量只有0.15左右的研究，要如何改進，才能確保能發現顯著差異，又有起碼80%的考驗力。圖\@ref(fig:shiny-distributions)是改良自Daniel Lakens釋出的Shiny app，用於解釋獨立組設計並以雙尾t檢定分析的研究，如何考慮樣本數目、效果量、p值、與考驗力。詳細請見圖中文字說明。

因為預設的樣本數是各組50人，顯著水準也是.05，所以你只要調整效果量到接近實驗結果的0.15，會看到考驗力只有11.52%。圖中說明也指出如果再做一次兩組都是50人的實驗，也只有11.52%的機率得到.05以下的*p*值。

至此你應該明白提高考驗力的重要性，大約10%的考驗力就是說除非你有資源執行各組50人的實驗大約十次，才有可能發現一次*p*值小於.05的結果。除非今天這個研究的效果量大於0.6，我們才有至少80%的考驗力。

如果無法提高效果量，另一個方法就是增加各組人數，請你親自試試看要增加到多少人，才能保障*p*值小於.05的實驗結果有至少80%的考驗力？

```{r shiny-distributions, echo=FALSE, message=FALSE, warning=FALSE, paged.print=TRUE, fig.cap="探索獨立組設計雙尾檢定的各項統計資訊關係。"}
knitr::include_app("https://scchen.shinyapps.io/p_distribution_in_stat_book/", 
  height = "2200px")
```

## 如何提高實驗結果的再現性？

前一段的互動展示帶來一個值得探討的問題：低效果量的獨立組研究是不是很難得到顯著結果？有顯著的結果是不是不太容易成功再現？如果你能想到這樣的問題，你已經有邁向進階課程的條件。

結束這個單元之前，我要使用心理科學家常用的考驗力分析軟體G\*Power[@FaulStatisticalpoweranalyses2009]，根據Wagenmakers等人的再現研究設定的顯著水準與獲得的效果量，製造樣本數與考驗力對應值資料。這筆資料的視覺化如同圖\@ref(fig:shiny-distributions)的左下角曲線圖，但是同時呈現獨立組設計與重覆量數設計的考驗力。

兩種設計的共同設定條件是效果量皆為0.149，起始樣本數是獨立組設計各組50，重覆量數設計總數50。G\*Power會根據共同條件估計橫跨樣本數50到3000的考驗力，你可以[由此下載](https://osf.io/dhmzk/)樣本數與考驗力的對應資料，圖\@ref(fig:power-curve)呈現兩種設計的樣本數與考驗力對應曲線，請想想看如果你有一個研究問題，能用獨立組設計，也可以用重覆量數設計收集資料，如果預期發現的效果量大約是0.149，那種設計會比較理想？

```{r power-curve, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE,out.width='100%',fig.cap="效果量是0.149，獨立組設計與重覆量數設計雙尾檢定的考驗力與樣本數對應曲線。"}
PowerCurve <- read.csv(file = "power_curve.csv")
levels(PowerCurve$Design) <- c("獨立組設計","重覆量數設計")
require(dplyr)
require(ggplot2)
PowerCurve %>% ggplot(aes(x=Size, y=Power)) + geom_point(aes(color=Design)) + labs(x="樣本數", y="考驗力")+ scale_color_discrete(name="設計") 
```


## 習題

開發中。
